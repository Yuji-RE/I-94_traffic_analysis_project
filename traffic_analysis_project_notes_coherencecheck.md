 1) データ品質・前処理（高優先）
- 明示的に parse_dates を使って CSV 読み込み時に date_time を datetime に変換（現在は後処理）。例: pd.read_csv(..., parse_dates=["date_time"]).
- 欠損値・外れ値を体系的に扱うワークフローを追加
  - temp==0 や rain_1h の極端値を検出→「ログに記録」→ケース別に除外 or 補正（センサ誤動作なら除外）を選択肢としてドキュメント化。
  - 2014/8/7〜2015/6/12 や 2016/7/23-24 等の欠損/異常区間はフラグを立てて分析時に除外 or 感度分析を行う。
- weather_description の正規化ルールを明文化（小文字化、同義語マージ：例 "sky is clear" と "sky is blue" を統合）。
- カテゴリの小さなグループは事前に閾値でまとめる（例、サンプル数 < N は "other" にまとめる）→平均の偏りを防ぐ。
- タイムゾーン / DST の確認（データが現地時間であるなら明記）。将来データ統合時のトラブル回避のためにタイムゾーン列を保持。

2) 統計的厳密性・検定設計（高優先）
- t検定の前に独立性と自己相関の問題を扱う：
  - 観測が時間系列かつ自己相関が残る可能性があるので、日時単位（日ごとの平均）などで集約して独立性を高める、またはブートストラップ/ランダム化検定を併用。
- 多重比較補正を導入（複数天候カテゴリを比較する場合、Holm/Benjamini–Hochberg等）。
- 効果量と信頼区間を必ず報告（Cohen's d や差の95%CI）。p値だけで判断しない。
- ロバスト性チェック：ノンパラ検定（Mann–Whitney, permutation test）、ブートストラップ CI、クラスタ頑健標準誤差（日・週でクラスタリング）を追加。
- 時系列特性が残るなら混合効果モデル（ランダム効果：日/週／年）や GEE、あるいはカウントデータ向けに（過分散を考慮して）ネガティブ・ビノミアル回帰を検討。

3) 時系列解析・周期性の扱い（中〜高）
- STL 分解（季節・傾向・残差）で季節性を分けてから比較（季節効果で天候と交通量の関係が歪むのを防ぐ）。
- ACF/PACF の解釈に加え、自己相関をモデルで明示的に扱う（ARIMA / SARIMA / GAM with cyclic splines）。
- heatmap・確率表作成の際は「絶対頻度」だけでなく「条件付き確率」P(weather | month,hour) と P(high_traffic | weather, month, hour) を出す。

4) モデル化・因果推論（中）
- マーケ施策に直接結びつく予測/期待値を作る：weather×month×hourの組合せごとの期待交通量（平均＋CI）とサンプル数を表にする。
- 単純比較だけでなく多変量回帰（例：traffic_volume ~ hour + weekday + month + weather + temp + holiday_flag）で共変量を調整。重要変数とその係数・予測効果を提示。
- 非線形関係は GAM（滑らかな時間・気温効果）で可視化すると説得力が出る。
- 因果主張をするなら、交絡に注意（イベント・工事・祝日・学校休み・道路閉鎖など）。可能なら外部データを取り込む。

5) 可視化・レポート（高）
- ==棒グラフにサンプルサイズを併記（棒の横に n を表示）。サンプル不均衡を可視化し、解釈の注意喚起を付加==。
- 棒/箱/バイオリンプロットで分布の広がりを示す。平均だけでなく分位点を提示。
- ヒートマップは「確率（P(weather | month,hour)）」＋「期待交通量（条件付き平均）」の2つを作る。色のスケールと凡例を見やすく。
- 重要な数値（差の大きさ、CI、n）を一枚の「意思決定用サマリ図（one-pager）」に集約する（マーケ向け）。
- すべての図をファイルに保存（PNG/SVG）し、NotebookではなくPDF/HTMLレポートも生成。

6) デリバラブル（マーケ部門向け） — 「使える形」で出す（最高優先）
- ==具体的な推奨カレンダーを作る：例えば表形式で「月」「曜日」「時間帯」「期待交通量」「scattered_prob」「haze_prob」「おすすめ（Yes/No）」を出力==。← **次回のレポート作成に含める**
- 推奨基準を明示（例：scattered_prob ≥ 0.15 AND haze_prob ≤ 0.02 AND expected_traffic ≥ baseline + X）。
- コストベネフィットのために「期待インプレッション増幅 = current_avg_traffic × (predicted_multiplier)」のような簡易計算式を付ける。
- 不確実性を表示（期待値の95%CI や最小/最大シナリオ）。

7) 追加データ・ドメイン知識（中）
- 外部データで検証：気象官署（NOAA/NWS）、道路イベント（closures/incidents）、祝日カレンダー、学校の休暇スケジュール、ローカル大イベント（コンサート等）。
- 可能なら複数年にわたる外部気象再解析データ（ERA5等）で weather_description の品質チェック。
- 交通センサーの仕様確認（観測単位、欠測・閾値）を担当者に問い合わせる。

8) コード品質・再現性（高）
- rpy2 に依存する理由は妥当だが、再現性のために Python-only（scipy.stats / statsmodels / pingouin / bootstrapped）による代替実装も用意すると配布が容易。
- ==notebook をモジュール化：主要処理（読み込み→前処理→集計→可視化→検定）を関数化して unit-test を少し追加==。← **EDA中にこれをやる具体的な方法を次回までに調べる**
- requirements.txt / environment.yml / Dockerfile を用意し、ランタイム再現性を確保。← **requirementsは今回含める。次回以降でDockerも含める**
- 結果を再利用しやすくするために、最終集計（weather×month×hour テーブル）を CSV/Parquet で保存。
- ロギング、バージョン（pandas 等）をレポートに記載。

9) 実行性能・スケーラビリティ（低→中）
- groupby 前に categorical dtype を使う（weather など）でメモリ軽減。
- 大きな集計は Pandas で問題ないが、将来スケールしたいなら dask / polars を検討。

10) ドキュメンテーション・意思決定トレーサビリティ（高）
- 「どの観測を除外したか」「どの正規化/変換を行ったか」を README に明記。
- 各主要結果（例：scattered vs haze のt検定）について仮定、手順、結果、解釈を短くまとめたセクションを作る（マーケが読める要約と、技術的な補遺の二段構成）。

最後に：マーケ向けの“アクション化”に必要な短い次の作業（すぐやるべき）
- 1) target_table を作る：全 (month, hour) に対して
    - P(scattered | month,hour), P(haze|...), P(mist|...)
    - expected_traffic (平均) と 95% CI
    - sample_size
    - 推奨フラグ（scattered_prob 高 & haze/mist_prob 低 & sample_size ≥ threshold）
  → CSV出力してマーケに渡す（可視化付き）。
- 2) 意思決定用 one-pager（1枚）作成：推奨時期（例 4–10月の特定月×時間帯）、期待増分、注意点（データ欠損・不確実性）。
- 3) 感度分析（外れ値・欠損を除外した場合の結果）を 1-2 グラフで付ける。


